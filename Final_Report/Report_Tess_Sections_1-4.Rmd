---
title: "ISYE-6414 Final Report - Air Quality Index"
author: "Ishika Arora, Ian Jiang, Tess Leggio"
date: "November 30, 2018"
output:
  pdf_document: default
  html_document: default
---
#0 - Problem Setup
```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Loading Script, include=FALSE}
library(MASS)
library(faraway)
library(nlme)
library(leaps)
library(Amelia)
library(pls)

#Loading data
# setwd("/Users/tessleggio/GoogleDrive/GeorgiaTech/2018-Fall/01-ISYE-6414/6414Project/cleaned_data")
data_raw <- read.csv("data_nomiss_specificAQI_updatedNov25.csv")
#Dropping missing values in response
data_raw <- data_raw[!is.na(data_raw$AQI),]
#Assigning date from year, month, day
data_raw$date <- as.numeric( difftime(ISOdate(data_raw$Year, data_raw$Month, data_raw$Day, 0), ISOdate(2016,1,1,0), units="days"))

#TEMPERATURE DATA
plot(data_raw$Temperature, data_raw$Month)
data_raw[data_raw$Temperature < -30,]

# remove iberville, louisiana since it is unlikely a county in LA actually saw 
#temperatures in the -38 range
remove <- c("iberville")
data1 <- data_raw
data1 <- data1[!(data1$County %in% remove),]

plot(data1$Temperature, data1$Month)
data1[which(data1$Temperature < -15 & data1$Temperature > -20 & data1$Month == 4),]


# remove effingham, illinois, greenup, kentucky, and roane, tennessee since 
#they have the exact same value for every day and is unlikely to have -17.78 
#in the middle of the summer
remove2 <- c("effingham", "greenup")
data2 <- data1[!(data1$County %in% remove2),]

data2[which(data2$Temperature < -15 & data2$Temperature > -20 & data2$Month == 7),]

remove3 <- c("roane")
data3 <- data2[!(data2$County %in% remove3),]

plot(data3$Temperature, data3$Month)

qqnorm(data3$Temperature)
qqline(data3$Temperature)
#probably needs transformation


#AQI_PM2.5
summary(data_raw$AQI_PM2.5)
plot(data_raw$AQI_PM2.5)
#all values within 0-500 range

qqnorm(data_raw$AQI_PM2.5)
qqline(data_raw$AQI_PM2.5)
#probably needs transformation


#AQI_PM10
summary(data_raw$AQI_PM10)
plot(data_raw$AQI_PM10, data_raw$Month)
#not all values within 0-500 range

#remove values which are outside of the AQI range of 0-500 since 
#something is not right with those values
data_raw[which(data_raw$AQI_PM10 > 500),]
remove4 <- c(8912, 13425)
data4 <- data3[-remove4,]
data4[which(data4$AQI_PM10 > 500),]

plot(data4$AQI_PM10, data4$Month)

qqnorm(data4$AQI_PM10)
qqline(data4$AQI_PM10)
#probably needs transformation

#AQI_CO
summary(data_raw$AQI_CO)
plot(data_raw$AQI_CO, data_raw$Month)
#all values within 0-500 range

qqnorm(data_raw$AQI_CO)
qqline(data_raw$AQI_CO)
#probably needs transformation


#AQI_NO2
summary(data_raw$AQI_NO2)
plot(data_raw$AQI_NO2, data_raw$Month)
#all values within 0-500 range

qqnorm(data_raw$AQI_NO2)
qqline(data_raw$AQI_NO2)
#probably needs transformation

#AQI_Ozone
summary(data_raw$AQI_Ozone)
plot(data_raw$AQI_Ozone, data_raw$Month)
#all values within 0-500 range

qqnorm(data_raw$AQI_Ozone)
qqline(data_raw$AQI_Ozone)
#probably needs transformation

#AQI_SO2
summary(data_raw$AQI_SO2)
plot(data_raw$AQI_SO2, data_raw$Month)
#all values within 0-500 range

qqnorm(data_raw$AQI_SO2)
qqline(data_raw$AQI_SO2)

qqnorm(data4$AQI_SO2)
qqline(data4$AQI_SO2)

data4[which(data4$AQI_SO2 == 200),]
#remove hawaii, hawaii since every single recorded day has the same AQI_SO2
remove5 <- c("hawaii")
data5 <- data4[!(data4$County %in% remove5),]

qqnorm(data5$AQI_SO2)
qqline(data5$AQI_SO2)
#probably needs transformation


#TOTAL EMISSIONS
summary(data_raw$Total.Emissions)
plot(data_raw$Total.Emissions, data_raw$Month)

data_raw[which(data_raw$Total.Emissions == max(data_raw$Total.Emissions)),]

#harris county (county which houses Houston) has much more total emissions 
#than the others, but no real reason to believe it's a bad data point

qqnorm(data5$Total.Emissions)
qqline(data5$Total.Emissions)
#probably needs transformation

#TESS FINAL DATA (NOT INCLUDING IAN'S REMOVALS)
data <- data5

data_PM2.5 <- data[!is.na(data$AQI_PM2.5),]
data_PM10 <- data[!is.na(data$AQI_PM10),]
data_CO <- data[!is.na(data$AQI_CO),]
data_NO2 <- data[!is.na(data$AQI_NO2),]
data_ozone <- data[!is.na(data$AQI_Ozone),]
data_SO2 <- data[!is.na(data$AQI_SO2),]

data_PM2.5 <- subset(data_PM2.5, select=-c(AQI))
colnames(data_PM2.5)[colnames(data_PM2.5)=="AQI_PM2.5"] <- "AQI"

data_PM10 <- subset(data_PM10, select=-c(AQI))
colnames(data_PM10)[colnames(data_PM10)=="AQI_PM10"] <- "AQI"

data_CO <- subset(data_CO, select=-c(AQI))
colnames(data_CO)[colnames(data_CO)=="AQI_CO"] <- "AQI"

data_NO2 <- subset(data_NO2, select=-c(AQI))
colnames(data_NO2)[colnames(data_NO2)=="AQI_NO2"] <- "AQI"

data_ozone <- subset(data_ozone, select=-c(AQI))
colnames(data_ozone)[colnames(data_ozone)=="AQI_Ozone"] <- "AQI"

data_SO2 <- subset(data_SO2, select=-c(AQI))
colnames(data_SO2)[colnames(data_SO2)=="AQI_SO2"] <- "AQI"

```

```{r Full Model, include=FALSE}
lmod <- lm(formula = AQI ~ NF3 + Other.GHG + Total.Emissions + HFC + Other.Fluorane + Biogenic.CO2 + Population + CO2 + PFC + HFE + Stationary.Combustion + pp_consumed_MMBtu + Temperature +  Methane + SF6 + Short.Lived.Compounds + Income +  pp_net_gen_MWh, data = data)

#Remove outliers and high leverage points
hatv <- hatvalues(lmod)
data_mod <- data[hatv < 0.2,]

stud <- rstudent(lmod)
data_mod <- data_mod[stud < 10,]
```

```{r NO2 model, include=FALSE}
lmod.NO2 <- lm(formula = AQI ~ NF3 + Other.GHG + Total.Emissions + HFC + Other.Fluorane + Biogenic.CO2 + Population + CO2 + PFC + HFE + Stationary.Combustion + pp_consumed_MMBtu + Temperature +  Methane + SF6 + Short.Lived.Compounds + Income +  pp_net_gen_MWh, data = data_NO2)

#Remove outliers and high leverage points
hatv <- hatvalues(lmod.NO2)
data_mod_NO2 <- data_NO2[hatv < 0.2,]

stud <- rstudent(lmod)
data_mod_NO2 <- data_mod_NO2[stud < 10,]
```

#1 - Problem Statement

Air quality is a factor that is cited as both contributing to and being affected by climate change (1), and has been shown to have a direct impact on human health. As a factor with such important consequences, our team decided to study factors that impact air quality. For our project, we focus on answering the following question: *How is air quality across the United States related to demographic, emissions, and weather factors?* Specifically, we chose to investigate the impact of factors related to economic output and climate, including temperature, population, per capita income, greenhouse gas emissions, and industrial power consumption and generation. Our response variable is the Air Quality Index (AQI), a measure of local air quality calculated by the Environmental Protection Agency (EPA) based on the concentrations of 6 major classes of ground-level pollutants: ozone, carbon monoxide, nitrogen dioxide, sulfur dioxide, and two measures of particulate matter (PM2.5 and PM10).

The scope of our analysis ultimately included county-level data for all US counties with available AQI data. To limit the size of our dataset to a size manageable within R, we used daily data from 2016, the most recent year with data available for each of the factors included in our analysis.

(1): https://www.epa.gov/air-research/air-quality-and-climate-change-research


#2 - Data Collection

Most of the data we used in our analysis is available from government websites. Data is available for download as CSV files from the following sources: per capita income data from bea.gov, population data from census.gov, powerplant energy consumption and generation data from eia.gov, and AQI data from aqs.epa.gov.

Weather data were significantly more challenging to collect. Daily weather sensor data is available from aqs.epa.gov, however the county data is far less complete than AQI data, so to avoid omitting a large portion of data from analysis, which could introduce bias into the model we instead collected temperature data from api.mesowest.net, an API which allows straight-forward programmatic extraction of data. By setting parameters in the URL, daily temperature and other weather averages can be scraped for any given day, state and county. We exploit this feature by looping through all counties in the AQI data and all days within our date range, sending a "get" request for each iteration. Data is available in JSON format, and includes daily averages for all stations within a given county. Therefore, a global measure of temperature and other weather factors is easily calculated by converting the JSON format into a Python dictionary and averaging across all stations. The speed of the script is highly dependent on local factors and internet speed, but on average it takes about 10 minutes per county for each of the 1053 counties included in our analysis.

#3 - Data Cleaning

Upon collecting data from each of the 6 identified data sources, we were faced with the task of merging the datasets together.  First and foremost, we had to ensure that the datasets merged together well. We had to merge data based on state name, county name, year, month, and date, so it was essential that these fields were in the exact same format across data sources to avoid loss of data.  In particular, we spent much time ensuring that county names were in the proper format.  Since county name is a string field, it was not uncommon to see several different formats (for example, Saint John vs. St. John vs. StJohn).  To avoid having several sparsely populated observations for each county, we wrote python scripts to format each dataset to use the same format and merge the datasets together.

After merging the datasets, we noted that there were several columns that were only sparsely populated, especially within weather data such as wind speed, relative humidity, and precipitation. As these data were sparse, we decided not to use them in our analysis because it may have introduced bias into the model.  As such, we dropped the columns from our dataset. The remaining dataset was large and did not have a large number of missing values, so we decided to omit NAs from analysis rather than imputing missing data.

Next, we checked our merged data to ensure data quality. One of our more effective methods for investigating data quality was to plot individual predictors versus time to easily spot any clear outliers.  Using this method, we were able to quickly spot clear outliers, such as the county Iberville, Louisiana, which reported temperatures of around -38C in the middle of the summer, which is not a reasonable value and did not align with checks on other websites.  These values and few others were thus identified as clear data quality issues and were omitted from analysis.

Finally, our initial dataset across multiple years included more than a million observations and was too large a file to easily work with within R, so our team made the decision to trim our dataset to a more reasonable size of approximately 330,000 observations by including only data from 2016.

#4 - Model Diagnostics (Ian)

In the following analysis, we give results for diagnostics preformed on one model, the model for AQI_NO2. The data for this model does not include records with missing or spurious values of the response or predictors. 

##Checking Error Assumptions

We check for the constancy of variance by examining the relationship of the residuals to the fitted values of the full model. If residuals follow a normal distribution, then their magnitudes would be half-normally distributed. Therefore, in order to mitigate the effects of the skewedness of this distribution, we also take the square root of the magnitudes, and plot it against the corresponding predictions:

```{r Residuals vs. Fitted, echo=FALSE}
 #Plot residuals against fitted
plot(lmod$fitted.values, sqrt(abs(lmod$residuals)), xlab="Fitted", ylab=expression(sqrt(hat(epsilon))), main="Residuals vs. Fitted")
```

When the square root residuals are fitted on the estimated response, we find a significant p-value, indicating that there is a dependence of the residuals. Therefore, transformation of either the response or factors are needed. For the moment, we leave the response alone, in order to proceed with the diagnostics.

##Checking Normality Assumption

Secondly, we investigate whether or not the residuals of the model are normal distributed. For this, we visually examine the histogram of residuals, as well as the qq-plot of residual quartiles against the corresponding Gaussian quartiles:

```{r Normal Testing, echo=FALSE}
par(mfrow=c(1, 2))

#Checking normality with QQ-plot
qqnorm(lmod$residuals, ylab="Residuals", main="QQ Plot of Residuals")
qqline(residuals(lmod))

#Checking normality with histogram of residuals
hist(residuals(lmod), xlab="Residuals", main="Histogram of Residuals")

```


From the qq-plot, the quartiles appear to be skewed, with mass tending to the tails. Additionally, the histogram shows that the residuals appear to be right-skewed. We also use the shapiro-wilks test:

```{r Shapiro, echo=FALSE}
#Checking normality with Shapiro-Wilks test
shapiro.test(lmod$residuals[1:4999])
```

The results reinforce the conclusion that residuals are non-normally distributed. 

##Checking for Serial Correlation

Next, we check the serial correlation in the error, to determine if there is any serial dependence of the residuals. This is achieved by simply regressing each residual on the previous, to determine if there is any trend:


```{r Serial correlation, echo=FALSE}
n <- length(lmod$residuals)

plot(tail(lmod$residuals, n - 1) ~ head(lmod$residuals, n - 1), xlab=expression(hat(epsilon)[i]), ylab=expression(hat(epsilon)[i + 1]))
summary(lm(tail(lmod$residuals, n - 1) ~ head(lmod$residuals, n - 1)))
```

The R squared and the p-values show that this result is significant, and we can conclude that there is marked autocorrelation among the residuals which may be caused by a missing covariate. 

#5 - Model Selection 
##A) stepwise selection
We tried running stepwise regression on the model before removing the collinear variables from the model, but the results were not very satisfactory. It removed only two factors out of the 20 predictors. However, when we looked at the pairwise correlation factors (and plots) and the VIFs, several factors seemed to be strongly correlated. This suggested that we had to remove more than just two factors from the model to address multicollinearity.

##B) Backward selection
Because of the complex interaction of the factors, we decided to use backward elimination for removing non-significant factors from the model. We started with 13 predictors in the model and then removed the predictor with highest p-value greater than 0.05. We removed two more factors from the model and finally used the following factors to train our model -  - *other greenhouse gases, hydrofluorocarbons, biogenic CO2, population, perfluorinated chemicals, hexafluorethane, stationary combustion, industrial power consumption, Temperature, sulfur hexafluoride, and per capita income*.


#6 - Transformations (Tess)



#7 - Final Model (Ishika)
Based on the diagnostics and transformation results (as shown above), we built the final models for each defining parameter by transforming the response as square root. We used the following set of factors to train the model, which were selected based on the results of multicollinearity tests in the diagnostics section - *nitrogen trifluoride, other greenhouse gases, hydrofluorocarbons, biogenic CO2, population, perfluorinated chemicals, hexafluorethane, stationary combustion, industrial power consumption, Temperature, methane, sulfur hexafluoride, and per capita income*.

The R-squared value of the full final model comes out to be 9.5%. Most of the predictors are statistically significant in this model as shown in the model summary below.


```{r Final Full Model}
lmod.T <- lm(formula = AQI ^ lambda ~ Other.GHG + HFC +  Biogenic.CO2 + Population + PFC + HFE + Stationary.Combustion + pp_consumed_MMBtu + Temperature + SF6 + Income, data = data_mod)

summary(lmod.T)
```

 
Similarly, we built the final component models for each defining parameter with response transformation and selected factors. We are only showing the results here for the NO2 model, for which the final R-squared value came out to be 20.5%.

```{r Final NO2 Model}
lmod.T.NO2 <- lm(formula = AQI ^ lambda ~ Other.GHG + HFC +  Biogenic.CO2 + Population + PFC + HFE + Stationary.Combustion + pp_consumed_MMBtu + Temperature + SF6 + Income, data = data_mod_NO2)

summary(lmod.T.NO2)
```

#8 - Future Research (Ishika)
##A) Additional predictors
Future research work can include additional predictors in the model, since only about 20% of the variation in the model is explained by the current set of predictors. Some additional factors that can be included are -
*more weather factors (wind speed, humidity and precipitation), geographical data like elevation or use city-level data rather than county-level data*.

##B) Additional scope
We can further investigate air quality with more data collected across the globe so that we have more informational dataset. We can also run the analysis over a larger time interval (current analysis is for an year's data). We couldn't process more than an year's data in R, so we decided to build the model for lesser data.

##C) Nonlinear models
We tried running the general additive model. Future work could involve expanding on this, since we think it would produce promising results.
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

