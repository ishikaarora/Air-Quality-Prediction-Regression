---
title: "aqs"
author: "Ian Jiang"
date: "November 24, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# rm(list=ls())
# cat("\014") 
# setwd("C:/Users/ijiang6/Desktop")
```

```{r}
install.packages("faraway")
install.packages("leaps")
install.packages("Amelia")
install.packages("pls")
```

```{r Loading in necessary packages}
#install.packages("MASS")
library(MASS)
#install.packages("faraway")
library(faraway)
#install.packages("nlme")
library(nlme)
library(leaps)
library(Amelia)
library(pls)
```

## Loading Data

```{r Loading raw data}
#Loading data
data_raw <- read.csv("data_nomiss_specificAQI_updatedNov25.csv")
#Dropping missing values in response
data_raw <- data_raw[!is.na(data_raw$AQI),]
#Assigning date from year, month, day
data_raw$date <- as.numeric( difftime(ISOdate(data_raw$Year, data_raw$Month, data_raw$Day, 0), ISOdate(2016,1,1,0), units="days"))
```

Here we remove spurious temperature and AQI data. 
```{r Data Cleaning}
#TEMPERATURE DATA
  plot(data_raw$Temperature, data_raw$Month)
  data_raw[data_raw$Temperature < -30,]
  
  # remove iberville, louisiana since it is unlikely a county in LA actually saw 
  #temperatures in the -38 range
    remove <- c("iberville")
    data1 <- data_raw
    data1 <- data1[!(data1$County %in% remove),]
  
  plot(data1$Temperature, data1$Month)
  data1[which(data1$Temperature < -15 & data1$Temperature > -20 & data1$Month == 4),]
  
  
  
  # remove effingham, illinois, greenup, kentucky, and roane, tennessee since 
  #they have the exact same value for every day and is unlikely to have -17.78 
  #in the middle of the summer
    remove2 <- c("effingham", "greenup")
    data2 <- data1[!(data1$County %in% remove2),]
  
    data2[which(data2$Temperature < -15 & data2$Temperature > -20 & data2$Month == 7),]
    
    remove3 <- c("roane")
    data3 <- data2[!(data2$County %in% remove3),]
    
  plot(data3$Temperature, data3$Month)
  
  qqnorm(data3$Temperature)
  qqline(data3$Temperature)
  #probably needs transformation


#AQI_PM2.5
  summary(data_raw$AQI_PM2.5)
  plot(data_raw$AQI_PM2.5)
  #all values within 0-500 range
  
  qqnorm(data_raw$AQI_PM2.5)
  qqline(data_raw$AQI_PM2.5)
  #probably needs transformation
  
  
#AQI_PM10
  summary(data_raw$AQI_PM10)
  plot(data_raw$AQI_PM10, data_raw$Month)
  #not all values within 0-500 range
  
  #remove values which are outside of the AQI range of 0-500 since 
  #something is not right with those values
    data_raw[which(data_raw$AQI_PM10 > 500),]
    remove4 <- c(8912, 13425)
    data4 <- data3[-remove4,]
    data4[which(data4$AQI_PM10 > 500),]

  plot(data4$AQI_PM10, data4$Month)
    
  qqnorm(data4$AQI_PM10)
  qqline(data4$AQI_PM10)
  #probably needs transformation

  
#AQI_CO
  summary(data_raw$AQI_CO)
  plot(data_raw$AQI_CO, data_raw$Month)
  #all values within 0-500 range
  
  qqnorm(data_raw$AQI_CO)
  qqline(data_raw$AQI_CO)
  #probably needs transformation
  
  
#AQI_NO2
  summary(data_raw$AQI_NO2)
  plot(data_raw$AQI_NO2, data_raw$Month)
  #all values within 0-500 range
  
  qqnorm(data_raw$AQI_NO2)
  qqline(data_raw$AQI_NO2)
  #probably needs transformation
  
#AQI_Ozone
  summary(data_raw$AQI_Ozone)
  plot(data_raw$AQI_Ozone, data_raw$Month)
  #all values within 0-500 range
  
  qqnorm(data_raw$AQI_Ozone)
  qqline(data_raw$AQI_Ozone)
  #probably needs transformation
  
#AQI_SO2
  summary(data_raw$AQI_SO2)
  plot(data_raw$AQI_SO2, data_raw$Month)
  #all values within 0-500 range
  
  qqnorm(data_raw$AQI_SO2)
  qqline(data_raw$AQI_SO2)
  
  qqnorm(data4$AQI_SO2)
  qqline(data4$AQI_SO2)
  
  data4[which(data4$AQI_SO2 == 200),]
  #remove hawaii, hawaii since every single recorded day has the same AQI_SO2
    remove5 <- c("hawaii")
    data5 <- data4[!(data4$County %in% remove5),]
  
  qqnorm(data5$AQI_SO2)
  qqline(data5$AQI_SO2)
  #probably needs transformation
  

#TOTAL EMISSIONS
  summary(data_raw$Total.Emissions)
  plot(data_raw$Total.Emissions, data_raw$Month)
  
  data_raw[which(data_raw$Total.Emissions == max(data_raw$Total.Emissions)),]
  
  #harris county (county which houses Houston) has much more total emissions 
  #than the others, but no real reason to believe it's a bad data point
  
  qqnorm(data5$Total.Emissions)
  qqline(data5$Total.Emissions)
  #probably needs transformation

  
#TESS FINAL DATA (NOT INCLUDING IAN'S REMOVALS)
data <- data5

```

## Fitting full model

```{r Full model}
lmod <- lm(formula = AQI ~ Nitrous.Oxide + NF3 + Other.GHG + Total.Emissions + HFC + Other.Fluorane + Biogenic.CO2 + Population + CO2 + PFC + HFE + Stationary.Combustion + pp_consumed_MMBtu + Temperature +  Methane + SF6 + Short.Lived.Compounds + Income +  pp_net_gen_MWh, data = data)

summary(lmod)

```

Here we regress the square root of the residuals, in order to estimate the effect of fitted values on the error. p-value is significant, but R2 is quite small (R2 ~ 0.006).
```{r Diagnostics}
#--------------------Constant Variance--------------------
#Plot residuals against fitted
plot(lmod$fitted.values, sqrt(abs(lmod$residuals)), xlab="Fitted", ylab=expression(sqrt(hat(epsilon))))
abline(h=0)

summary(lm(sqrt(abs(residuals(lmod))) ~ fitted(lmod)))


```

Motivation for splitting analysis into 6.
```{r}
plot(data$Defining.Parameter,residuals(lmod), xlab="Defining Parameter",ylab="Residuals")
table(data$Defining.Parameter)
```

Shapiro Wilks says that data is not normal; there appers to be some skewed data. 
```{r normality}
#--------------------Normality--------------------
#Checking normality with QQ-plot
qqnorm(lmod$residuals, ylab="Residuals", main="")
qqline(residuals(lmod))

#Checking normality with histogram of residuals
hist(residuals(lmod), xlab="Residuals", main="")

#Checking normality with Shapiro-Wilks test
shapiro.test(lmod$residuals[1:4999])
```

The null hypothesis is that the the residuals are normal. Since the p-value is small, we can reject this hypothesis. This means that the residuals are not normal.

With a large dataset, even mild deviations from non-normality may be detected, but there would be little reason to abandon least squares because the effects of non-normality are mitigated by large sample sizes. For smaller sample sizes, formal tests lack power.

Testing for serial correlation -- appears to be significant correlation.
```{r Timeseries Diagnostics}

#--------------------Timeseries Diagnostics--------------------
#Checking for serial correlation 
n <- length(lmod$residuals)
plot(tail(lmod$residuals, n - 1) ~ head(lmod$residuals, n - 1), xlab=expression(hat(epsilon)[i]), ylab=expression(hat(epsilon)[i + 1]))
summary(lm(tail(lmod$residuals, n - 1) ~ head(lmod$residuals, n - 1)))
```
#Check for large leverage points
```{r Leverage and outliers}
#--------------------Checking Leverage Points--------------------
hatv <- hatvalues(lmod)
halfnorm(hatv, labs=data$County, ylab="Leverages")
head(hatv)

sum(hatv > 0.2)

data_mod <- data[hatv < 0.2,]
```



```{r Checking for Outliers}

#--------------------Checking for Outliers--------------------
stud <- rstudent(lmod2)
data_mod <- data_mod[stud < 10,]

```

```{r Fitting new model}
lmod2 <- lm(formula = AQI ~ Nitrous.Oxide + NF3 + Other.GHG + Total.Emissions + HFC + Other.Fluorane + Biogenic.CO2 + Population + CO2 + PFC + HFE + Stationary.Combustion + pp_consumed_MMBtu + Temperature +  Methane + SF6 + Short.Lived.Compounds + Income +  pp_net_gen_MWh, data = data_mod)

summary(lmod2)

```

Errors appear to be approximately constant, by examination of the residuals vs. fitted. However, the histogram, QQ-plots, and Shapiro-Wilks indicate that the data is strongly non-normal. Data transformations are clearly required. Additionally, residuals display moderate serial correlation (R2 = 0.45 for a serial regression). Generalized least squares can resolve this issue. 

Furthermore, we find that Pinal County (Arizona) represents an extreme outlier on 7/29/2016 (from its large studentized error). Its removal, however, does not significantly affect the model, since the dataset is large.

```{r Checking for multicollinearity}

#Generating condition numbers
x <- model.matrix(lmod2)[,-1]
e <- eigen(t(x) %*% x)
sqrt(e$val[1]/e$val)

#Checking variance inflation factors
vif(x)

#Generating correlation matrix
cor(data[,c("AQI", "HFC", "Other.GHG", "SF6", "Stationary.Combustion", "Biogenic.CO2", "HFE", "NF3", "PFC", "Short.Lived.Compounds", "Temperature", "pp_consumed_MMBtu", "CO2", "Income", "Nitrous.Oxide", "Population", "Total.Emissions", "pp_net_gen_MWh", "Methane", "Other.Fluorane")])

#Removing significantly collinear columns from the VIFs

```
  
 Checking multicollinearity with condition numbers (large numbers >30 signify collinearity). After that we address the pairwise correlations between the predictors. 
  
```{r Checking for multicollinearity}
lmod3 <- lm(formula = AQI ~ Biogenic.CO2 + Population + Stationary.Combustion + pp_consumed_MMBtu + Temperature + Short.Lived.Compounds + Income + pp_net_gen_MWh, data = data_mod)

summary(lmod3)
```

NOTE - gives an error with non-positive values for AQI. Fix it maybe!!
```{r Checking for multicollinearity}
lmod4 <- lm(formula = AQI ~ Biogenic.CO2 + Population + Stationary.Combustion + pp_consumed_MMBtu + Temperature + Total.Emissions + Income + pp_net_gen_MWh, data = data_mod)

summary(lmod4)
```

```{r Data Transformations}

#Making response AQI positive by adding a small constant (1) -- can change later
data_mod$AQI <- data_mod$AQI + 1

#Getting model Likelihoods on a range of parameters
bx <- boxcox(lmod4, plotit=T, lambda=seq(0.0, 0.5, by=0.001))

#Getting best boxcox parameter -- lambda ~ 0.5
lambda <- bx$x[which.max(bx$y)]

#Transforming model accordingly
full.lmod.T <- lm(formula = AQI ^ lambda ~ Biogenic.CO2 + Population + Stationary.Combustion + pp_consumed_MMBtu + Temperature + Total.Emissions + Income + pp_net_gen_MWh, data = data_mod)
```

```{r}
summary(full.lmod.T)
```
```{r}
#--------------------Normality--------------------
#Checking normality with QQ-plot
qqnorm(full.lmod.T$residuals, ylab="Residuals", main="")
qqline(residuals(full.lmod.T))

#Checking normality with histogram of residuals
hist(residuals(full.lmod.T), xlab="Residuals", main="")

#Checking normality with Shapiro-Wilks test
shapiro.test(full.lmod.T$residuals[1:4999])
```



Based on the above distribution, we decided to build separate models based on "Defining Parameter" value.

```{r split up data by response}
data_PM2.5 <- data_mod[!is.na(data_mod$AQI_PM2.5),]
data_PM10 <- data_mod[!is.na(data_mod$AQI_PM10),]
data_CO <- data_mod[!is.na(data_mod$AQI_CO),]
data_NO2 <- data_mod[!is.na(data_mod$AQI_NO2),]
data_ozone <- data_mod[!is.na(data_mod$AQI_Ozone),]
data_SO2 <- data_mod[!is.na(data_mod$AQI_SO2),]
```

## Different models based on Defining Parameter

### Defining Parameter = Ozone

```{r ozone full}
ozone.lmod <- lm(formula = AQI_Ozone ~ Biogenic.CO2 + Population + Stationary.Combustion + pp_consumed_MMBtu + poly(Temperature,2) + Total.Emissions + Income + pp_net_gen_MWh, data = data_ozone)

summary(ozone.lmod)



```

```{r}
#--------------------Normality--------------------
#Checking normality with QQ-plot
qqnorm(ozone.lmod$residuals, ylab="Residuals", main="")
qqline(residuals(ozone.lmod))

#Checking normality with histogram of residuals
hist(residuals(ozone.lmod), xlab="Residuals", main="")

#Checking normality with Shapiro-Wilks test
shapiro.test(ozone.lmod$residuals[1:4999])
```

### Defining Parameter = PM.25

```{r pm25 full}
pm25.lmod <- lm(formula = AQI_PM2.5  ~ Biogenic.CO2 + Population + Stationary.Combustion + pp_consumed_MMBtu + Temperature + Total.Emissions + Income + pp_net_gen_MWh, data = data_PM2.5)

summary(pm25.lmod)
```


### Defining Parameter = PM.10

```{r pm10 full}
pm10.lmod <- lm(formula = AQI_PM10 ~ Biogenic.CO2 + Population + Stationary.Combustion + pp_consumed_MMBtu + Temperature + Total.Emissions + Income + pp_net_gen_MWh, data = data_PM10)

summary(pm10.lmod)
```

### Defining Parameter = so2

Note - commented out temporarilyfor new data - since it fails "Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : 
  0 (non-NA) cases"
  
```{r so2 full}
so2.lmod <- lm(formula = AQI_SO2  ~ Biogenic.CO2 + Population + Stationary.Combustion + pp_consumed_MMBtu + Temperature + Total.Emissions + Income + pp_net_gen_MWh, data = data_SO2)

summary(so2.lmod)
```

### Defining Parameter = no2

```{r no2 full}
no2.lmod <- lm(formula = AQI_NO2  ~ Biogenic.CO2 + Population + Stationary.Combustion + pp_consumed_MMBtu + Temperature + Total.Emissions + Income + pp_net_gen_MWh, data = data_NO2)

summary(no2.lmod)
```

### Defining Parameter = co

```{r co full}
co.lmod <- lm(formula = AQI_CO  ~ Biogenic.CO2 + Population + Stationary.Combustion + pp_consumed_MMBtu + Temperature + Total.Emissions + Income + pp_net_gen_MWh, data = data_CO)

summary(co.lmod)
```


Although the likelihood is maximized by this value of lambda, we find that the R2 value is still small :( 


```{r Generalized Least Squares}
full.glmod <- gls(AQI ~ Nitrous.Oxide + NF3 + Other.GHG + Total.Emissions + HFC + Other.Fluorane + 
+ Biogenic.CO2 + Population + CO2 + PFC + HFE + Stationary.Combustion + pp_consumed_MMBtu + Temperature +  Methane + SF6 + Short.Lived.Compounds + Income +  pp_net_gen_MWh,  data = data)
```

THE END!!!